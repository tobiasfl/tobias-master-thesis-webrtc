\documentclass[UKenglish]{ifimaster}
\usepackage[utf8]{inputenc}           %% ... or latin1
\usepackage[T1]{fontenc,url}
\urlstyle{sf}
\usepackage{babel,textcomp,csquotes,duomasterforside,varioref,graphicx}
\usepackage[backend=biber,style=numeric-comp]{biblatex}
\usepackage[hidelinks]{hyperref}


\title{Awesome title here}       
\subtitle{Managing Real- Time Video and Data Flows with Coupled Congestion Control Mechanism}
\author{Tobias Fladby}                     

\addbibresource{./bibdb/mybib.bib}           
\addbibresource{./bibdb/allrefs.bib}

\begin{document}
\duoforside[dept={Department of Informatics},   %% ... or your department
  program={programming and system architecture},  %% ... or your programme
  long]                                       

\frontmatter{}
\chapter*{Abstract}            

\tableofcontents{}
\listoffigures{}
\listoftables{}

\chapter*{Preface}              

\mainmatter{}
\chapter{Introduction}              

\section{Problem statement}
\section{Contributions}
\section{Research questions}
\emph{Overall:}
\begin{itemize}
    \item Can two heterogenous control mechanisms be coupled? Will it improve overall performance?
\end{itemize}
\emph{Simplicity:}
\begin{itemize}
    \item Can such a mechanism be designed simple enough for widespread implementation? Moreover can it be easily integrated with other congestion control mechanisms? 
\end{itemize}
\emph{Fairness:}
\begin{itemize}
    \item Will both flows get their allocated share of bandwidth when needed?
    \item Will the coupled flows be fair to other flows sharing the same bottleneck?
    \item Will the bandwidth be shared according to configured priority?
\end{itemize}
\emph{Delay:}
\begin{itemize}
    \item Can it reduce delay spikes?
\end{itemize}
\emph{Link utilization:}
\begin{itemize}
    \item Will link utilization be equal to a single flow using the full link?
\end{itemize}
\emph{Responsiveness:}
\begin{itemize}
    \item Will any of the congestion control mechanisms be more responsive to congestion in the network?
\end{itemize}
\emph{Packet loss:}
\begin{itemize}
    \item Will any of the flows experience less packet loss?
    \item Can it reduce packet loss spikes for the flows?
\end{itemize}

\section{Organization}

\chapter{Background}

\section{WebRTC architecture}
\subsection{Real- time communication}
\subsection{Standardization}
\subsection{Protocol Stack}
\subsection{User API}
\subsubsection{Services}
\subsubsection{RTCPeerConnection API}
\subsubsection{DataChannel API}
\subsection{Signalling}
\subsubsection{NAT}
\subsubsection{ICE Framework}
\subsubsection{TURN}
\subsubsection{STUN}
\subsubsection{SDP}
\subsection{Encryption}
\subsubsection{TLS}
\subsubsection{DTLS}
\subsection{Usage}
\subsection{Browser engine}
\subsubsection{Aquiring WebRTC statistics}

\section{Transport protocols}
\subsection{TCP}
\subsection{UDP}
\subsubsection{Message- oriented protocols}
\subsection{RTP and RTCP}
\subsection{SCTP}
SCTP is a transport protocol offering many of the same services as TCP while also bringing additional features and improvements to the table. 
SCTP offers a point- to- point connection- oriented reliable delivery service while also using the same flow and congestion control algorithms as TCP.   %cite RFC perhaps
As opposed to TCP, SCTP is message- oriented. %explain more
An SCTP connection is called an association.

SCTP separates application data into chunks, each identified by a separate chunk header. 
These chunks are bundled into a single SCTP message that consists of an SCTP message header followed by several data chunks.
A key feature here is that the data chunks are independently identified with a separate header, thus a single SCTP message can contain data from separate streams of application data. For example one stream being text messages and another being the transfer of a file in a messaging application.
The advantage of this packet structure is that it means SCTP can support multi- streaming since it can send multiple data streams in parallel through a single SCTP association. 
%TODO: have a small section explaining head- of- line blocking and how SCTP avoids it 

Multi- streaming means that an application can transmit several independent streams of data in parallel. 

SCTP separates application data into chunks, each identified by a separate header. 
A single SCTP packet can contain several data chunks from different application data streams.

%TODO:cite some of the papers that have shown SCTP improvements compared to TCP 

\section{Congestion control}
\subsection{Loss- based congestion control}
\subsection{Delay- based congestion control}
\section{WebRTC Congestion controls}
Video data by nature is large in size so transmitting it creates a lot of traffic. 
This makes real- time communication challenging because it requires low latency in order to assure a good user experience. 

History and previous research [cite relevant stuff, like congestion collapse]has shown that protocols should employ mechanisms that limit the amount of data sent per second to a reasonable level in order to avoid congestion as well as keep the latency low.
\subsection{Google Congestion Control}
RTP by itself only provides simple end- to- end delivery services for multimedia[cite RTP standard], since real- time communication requires congestion control it must implemented on top of RTP. 
Chromium's WebRTC implementation uses an algorithm called Google Congestion Control \cite{draft-ietf-rmcat-gcc} to provide the mechanism.
It consists of two controllers, one loss- based and one delay- based. 
The loss- based controller located on the sender- side, uses loss rate, RTT and REMB[Cite REMB message definition] messages to compute a target sending bitrate. 
The delay- based controller can either be implemented on the receiver- side or sender- side.
It uses packet arrival info to compute a maximum bitrate which is passed to the loss- based controller. The actual sending rate is set to the minimum of the two bitrates.
% Might want to explain RTP terms like groups of packets and etc. 
\subsubsection{The loss- based controller}
The loss- based controller is run every time a feedback message from the receiver- side is received. 
If more than 10\% of packets have been lost when feedback is received the controller decreases the estimate. 
If less than 2\% is lost it will increase the estimate under the presumption that there is more bandwidth to utilize. 
Otherwise the estimate stays the same.
\subsubsection{The delay- based controller}
 The delay- based controller consists of several parts: pre- filtering, an arrival- time filter, an over- use detector and a rate controller. %% probably smart to have a figure of this

 Pre- filtering is used to make sure that channel outages, events unrelated to congestion are not interpreted as congestion.
Packets will naturally be delayed when a channel outage occurs so without this filter the algorithm would unnecessarily lower bitrate, thus lowering the quality of the communication for no reason.
A channel outage will cause the packets to be queued in network buffers, thus when the channel is restored the packets will arrive in bursts. 
The filter utilizes the fact that the packet groups will arrive in bursts during a channel outage and merges them under such conditions.

The arrival- time filter is responsible for calculating the queueing time variation which is an estimation of how the delay is developing at a certain time.
%TODO: fill this one with more info
The goal of the over- use detector is to produce a signal that drives the state of the remote rate controller. 
The goal of the over- use detector is to compare the queueing time variation obtained as output from the arrival- time filter with a threshold. If the estimate is above the threshold for a certain amount of time and not sinking it wil signal the rate contol.
%TODO: incorporate explaination of the remote rate controller as well as the related state machine and how it works
\subsubsection{Performance}
\subsection{NADA}
%Sytem overview
\subsubsection{Overview}
\subsubsection{Receiver agent}

\subsubsection{Sender agent}

\subsection{SCReAM}


\section{Coupled Congestion Control}
\subsection{Problems with combined controls}
\paragraph{The problem}
There are inherent differences in how quickly different types of congestion control react to congestion and combining them can therefore easily lead to unintentional side- effects. 
This is a known issue especially when it comes to combining loss- based controls with delay- based controls.
It has been shown to lead to a competition between the flows resulting in spikes in queueing delay and packet loss. 
%TODO: Any sources to cite for this, e.g. in regards to WebRTC?
The main issue stems from the fact that delay always happens earlier than loss.
The reason is that the first thing that happens when there is congestions is that the bottleneck queues will start filling, thus making packet more delayed, but not necessarily dropped until the queue is full or close to full.
%TODO: some article showing problems with combining loss-based and delay- based
Intuitively, this means that delay is observable earlier than loss when there is congestion.
The consequence of all this is that the delay- based control will decrease it's sending rate sooner than the loss- based will. 
Such behaviour leads to a very bad dynamic where the delay- based control lowers the bitrate at such an early stage of congestion that the loss- based never experiences packet loss and thus keeps increasing its send rate. 
The final result is that the delay- based control will get a smaller and smaller share of the available bandwidth because the loss- based control keeps increasing while the delay- based keeps backing down because of the congestion caused by the still- increasing traffic from the loss- based controller.
\paragraph{Coupling the controls}
A possible solution is to have the controllers cooperate by sharing their information, given that they are both located on the same host and are travelling the same path across the network i.e. coupling the flows.
Firstly, such a mechanism could benefit both controllers by giving them more information, and different types of information to base their decisions on. 
Secondly this could be used to ensure that the loss- based algorithm is fair to the delay- based.
One could for instance make sure that the loss- based control also decreases when the delay- based controller decreases the send rate.
One mechanism for coupling is "The Congestion Manager" (CM) \cite{rfc3124}.
CM couples flows by offering a single congestion controller instead.
The downside is that it is considered quite hard to implement because it requires an extra congestion controller and strips away all per- connection congestion control functionality, which is a drastic change.
%TODO: Should actually read RFC3124 yourself
%TODO: Maybe discuss Ensemble TCP?
A newer solution called "Coupled Congestion Control" \cite{rfc8699} combines congestion controls travelling over the same bottleneck while at the same time aiming at being easier to implement than the congestion manager.
As opposed to The Congestion Manager, Coupled Congestion Control tries to utilize the congestion control algorithms of the coupled flows by sharing the information they gather among them instead of completely removing them. 
The mechanism has aready shown promise in \cite{10.1145/2740070.2630089, 7502803} when implemented with homogenous congestion controls but has not been tested on heterogenous congestion controls. 
%TODO: read about how it performs and cite/mention it around here
%TODO: Explain why this could prove useful for WebRTC?
\paragraph{Coupled Congestion Control Architecture}
The design philosophy of Coupled Congestion Control is that the amount of required changes to existing applications should be minimal. 
The system consists of three elements, Shared Bottleneck Detection(SBD), Flow State Exchange(FSE) and the flows. 

\paragraph{Managing flows}

When a flow starts it will register itself with the FSE and SBD, when it stops it will deregister from the FSE and carry out an UPDATE function call every time their congestion controller calculates a new rate.
When a flow registers itself the SBD will assign it to a Flow Group by giving it a Flow Group Identifier.
A flow group is defined as a group of flows that share the same bottleneck and thus should exchange information with each other. 
The SBD is responsible for reassigning a flow to a different FG whenever the bottleneck changes.
\subsection{The Flow State Exchange}
The FSE can be described as a manager that maintains information exchanged between the flows and calculates a bit rate for each flow based on all the information gathered. 

It can be implemented in two ways: \textit{active} or \textit{passive}.
In the active version, the FSE will actively initiate communication with each flow and SBD. 
While in the passive version it does not actively initiate communication and only has the task of internal state maintenance.
The FSE keeps a list of all flows that have registered with it.
For each flow the FSE will store the following:
\begin{itemize}
    \item A unique number f to identify the flow.
    \item The Flow Group Identifier (FGI).
    \item The priority value P(f).
    \item The rate used by the flow which is calculted by the FSE in bits per second FSE\_R(f).
    \item The desired rate of the flow, DR(f).
\end{itemize}

The priority value P is used to calculate the flow's priority portion out of the sum of all priority values.
The desired rate might be smaller than the calculated rate, e.g. because the application wants to limit the flow or simply does not have enough data to send. 
If there is no desired rate value given by the flow it should just be set to the sending rate provided by the flows congestion control.

For each FG the FSE keeps a few static variables:
\begin{itemize}
    \item The sum S\_CR of calculated rates for all flows in th FG.
    \item The sum S\_P of all priorities in the FG.
    \item The total leftover rate TLO. This is the sum of leftover rate by rates limited by desired rate.
    \item Aggregate rate AR given to flows that are not limited by desired rate. 
\end{itemize}

Every time a flow's congestion control normally would update the flow's rate they carry out an UPDATE call to FSE instead. 
Through the UPDATE call they provide their newly calculated rate and optionally a desired rate. 
Then FSE calculates rates for all the flows and sends them back. 
When a flow f starts, FSE\_R is initialized with the initial rate calculated by f's congestion controller. 
After the SBD assigns the flow to an FG, it adds its FSE\_R to S\_CR.
The desired rate is smaller than the calculated rate when the flow is limited by an application, otherwise it will be the same as the calculated rate.

\paragraph{Active FSE}
In the active version FSE recalculates rates and notifies all the other flows in the FG as well whenever there is an UPDATE call from a single flow. 

% What happens when a flow start
% What happens when a flow UPDATES

In \cite{rfc8699} there are two examples of active FSE algorithms outlined.
The first algorithm %TODO
The second algorithm which improves upon %TODO 

\paragraph{Passive FSE}
In the passive version of FSE it only updates the sending rate for the flow that makes the UPDATE call and not the rest, as opposed to active FSE.
\section{Shared Bottleneck Detection}
The SBD is an entity that is responsible for determining which flows are traversing the same bottleneck. 
In \cite{rfc8699} three methods for deriving if flows share the same bottleneck are mentioned.
\subsection{Multiplexed flows}
One way is through comparing multiplexed flows. 
%TODO: might want to explain the term five- tuple
Since the flows with the same five- tuple will be routed along the same path, SBD can assume that they share the same bottleneck. 
However this method cannot be used for coupled congestion controllers with one sender talking to multiple receivers, given that they will not have the same five- tuple. 
Since WebRTC uses both SRTP and SCTP multiplexed on UDP, this implies they have the same five- tuple and that the first method will work. 
\subsection{Measurement}
One might also use measurements of e.g. delay and loss and look at correlations to derive if flows have a shared bottleneck.
\subsection{Configuration}


\chapter{Design}

\chapter{Implementation}                 


\chapter{Evaluation}
\section{Testbed}
\section{Experiments}

\chapter{Conclusion}                    
\section{Research Findings}
\section{Further work}
\section{Closing remarks}

\backmatter{}
\printbibliography
\end{document}
